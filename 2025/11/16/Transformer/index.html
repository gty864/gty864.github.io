<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer | Welcome to GTY's Blog</title><meta name="author" content="GTY"><meta name="copyright" content="GTY"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="TransformerWord Embedding每一个单词都可以表示成n维向量的形式，这样我们就可以用计算机来处理文本了。因此通俗来说，word embedding就是一个把输入文本转化成词向量矩阵的一个操作。 假设我们有一个句子：”I love machine learning”，经过tokenizer处理后，得到4个token，分别是”I”、”love”、”machine”、”learnin">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="http://gty864.github.io/2025/11/16/Transformer/index.html">
<meta property="og:site_name" content="Welcome to GTY&#39;s Blog">
<meta property="og:description" content="TransformerWord Embedding每一个单词都可以表示成n维向量的形式，这样我们就可以用计算机来处理文本了。因此通俗来说，word embedding就是一个把输入文本转化成词向量矩阵的一个操作。 假设我们有一个句子：”I love machine learning”，经过tokenizer处理后，得到4个token，分别是”I”、”love”、”machine”、”learnin">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://gty864.github.io/img/avatar.jpg">
<meta property="article:published_time" content="2025-11-17T05:35:37.000Z">
<meta property="article:modified_time" content="2025-12-01T08:35:08.425Z">
<meta property="article:author" content="GTY">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://gty864.github.io/img/avatar.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer",
  "url": "http://gty864.github.io/2025/11/16/Transformer/",
  "image": "http://gty864.github.io/img/avatar.jpg",
  "datePublished": "2025-11-17T05:35:37.000Z",
  "dateModified": "2025-12-01T08:35:08.425Z",
  "author": [
    {
      "@type": "Person",
      "name": "GTY",
      "url": "http://gty864.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://gty864.github.io/2025/11/16/Transformer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" referrerpolicy="no-referrer" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Welcome to GTY's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformer</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-11-17T05:35:37.000Z" title="Created 2025-11-16 21:35:37">2025-11-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-12-01T08:35:08.425Z" title="Updated 2025-12-01 00:35:08">2025-12-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><p>每一个单词都可以表示成n维向量的形式，这样我们就可以用计算机来处理文本了。因此通俗来说，word embedding就是一个把输入文本转化成词向量矩阵的一个操作。</p>
<p>假设我们有一个句子：”I love machine learning”，经过tokenizer处理后，得到4个token，分别是”I”、”love”、”machine”、”learning”。然后我们就可以利用word embedding把四个token转化成四个词向量拼接而成的矩阵。假设每个token的词向量维度是512，那么经过word embedding后，我们会得到一个4x512的矩阵X</p>
<p>在预处理的时候，我们会通过学习得到一个vocabulary embedding的矩阵作为词表，然后利用</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>X</mi><mo>=</mo><msub><mi mathvariant="normal">Ω</mi><mi>e</mi></msub><mi>T</mi></mrow><annotation encoding="application/x-tex">
X = \Omega_e T
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">Ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span></span>
<p>得到最终的word embedding矩阵X. 这个矩阵操作就相当于在vocabulary embedding里面寻找input token的位置，然后提取出来再拼接成矩阵X.(在上面的例子中，就是找I,love,machine,learning四个词在vocabulary embedding中的位置，然后提取出来再拼接成矩阵X) 其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Ω</mi><mi>e</mi></msub></mrow><annotation encoding="application/x-tex"> \Omega_e </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">Ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是vocabulary embedding, T是token indices, 每列只有input token在vocabulary embedding的位置处才是1，其余都是0. 用来从vocabulary embedding里提取input里每一个token的词向量。</p>
<p>下图展示了这一过程，其中D是词向量维度，N是input的token数，V是词表大小.<br><img src="/img/word_embedding.jpg" alt="image"></p>
<p>在BERT中，我们采用上下文词嵌入(contexual embedding), 即每个token的词向量不是简单的从vocabulary embedding中提取，而是根据上下文来学习得到的。</p>
<h2 id="Positional-encoding"><a href="#Positional-encoding" class="headerlink" title="Positional encoding"></a>Positional encoding</h2><p>为什么需要positional encoding? </p>
<p>在文本中，显然相同token在不同位置的含义是不同的,举个例子<br>“我爱学习”<br>“学习爱我”<br>在这两个句子中，”我”，这个 token 所处位置不同，含义也是不同的。</p>
<p>但self-attention 不知道序列中每个 token 的位置信息，模型会把上面两个句子当作一样的。所以这个时候需要 positional encoding 来给模型提供位置信息。</p>
<p>具体操作是，给每个 token 一个唯一的位置编码，然后把位置编码加到 token 的 embedding 上。</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>X</mi><mo>=</mo><mi>X</mi><mo>+</mo><mi mathvariant="normal">Ω</mi></mrow><annotation encoding="application/x-tex">X=X+\Omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Ω</span></span></span></span></span>
<p>其中 X 是所有 token 的 embedding 矩阵，Ω 是位置编码矩阵。</p>
<p>那么怎么获得这个位置编码矩阵 Ω 呢？ 可以通过直接添加一个预设好的位置编码矩阵，或者通过学习得到一个位置编码矩阵。</p>
<p>上面说的这个方法是传统的 positional encoding 方法，称为absolute positional encoding。它的缺点是只能表示绝对位置，即只能表示“这个词在句子中是第 5 位”，不能表示“这个词与另一个词相距 5 个 token”。</p>
<p>所以，我们可以使用 Relative Positional Encoding的方法做位置编码，核心思想是让模型学习 token i 和 token j 之间的相对距离（j – i）对注意力的影响，而不是 token 的绝对位置。</p>
<p>也就是说 attention 不只是 Q * K，还加入一个项： position i 到 position j 的相对位置 embedding。<br>	<br>以下是三种主流relative positional encoding 的公式</p>
<h3 id="Transformer-XL-类型"><a href="#Transformer-XL-类型" class="headerlink" title="Transformer-XL 类型"></a>Transformer-XL 类型</h3><p>在 attention score 中显式加入 R_(i-j)</p>
<p>传统 attention：</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub><mi>j</mi><mo>=</mo><msubsup><mi>Q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi>K</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">A_ij = Q_i^T K_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1774em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span>


<p>加入相对位置后：</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub><mi>j</mi><mo>=</mo><msubsup><mi>Q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi>K</mi><mi>j</mi></msub><mo>+</mo><msubsup><mi>Q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi>R</mi><mo stretchy="false">(</mo></msub><mi>i</mi><mo>−</mo><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><msup><mi>u</mi><mi>T</mi></msup><msub><mi>K</mi><mi>j</mi></msub><mo>+</mo><msup><mi>v</mi><mi>T</mi></msup><msub><mi>R</mi><mo stretchy="false">(</mo></msub><mi>i</mi><mo>−</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A_ij = Q_i^T K_j
      + Q_i^T R_(i-j)
      + u^T K_j
      + v^T R_(i-j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1774em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2465em;vertical-align:-0.3552em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">(</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1774em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2465em;vertical-align:-0.3552em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mopen mtight">(</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span></span></span></span></span>


<p>其中：</p>
<p>R_(i-j)：表示 token i 到 token j 的相对距离的 embedding</p>
<p>u, v：全局可学习偏置</p>
<p>i - j：token 之间距离</p>
<h3 id="T5-的-Relative-Position-Bias"><a href="#T5-的-Relative-Position-Bias" class="headerlink" title="T5 的 Relative Position Bias"></a>T5 的 Relative Position Bias</h3><p>T5 方法更简洁,只添加一个可学习的相对位置偏置项,不修改 Key &#x2F; Query：</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub><mi>j</mi><mo>=</mo><msubsup><mi>Q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi>K</mi><mi>j</mi></msub><mo>+</mo><msub><mi>b</mi><mi>r</mi></msub><mi>e</mi><mi>l</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A_ij = Q_i^T K_j + b_relative(i,j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1774em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span></span></span></span></span>




<h3 id="RoPE：Rotary-Positional-Embedding（GPT-LLaMA-主流使用）"><a href="#RoPE：Rotary-Positional-Embedding（GPT-LLaMA-主流使用）" class="headerlink" title="RoPE：Rotary Positional Embedding（GPT&#x2F;LLaMA 主流使用）"></a>RoPE：Rotary Positional Embedding（GPT&#x2F;LLaMA 主流使用）</h3><p>将 token 的 embedding 做一个角度旋转：</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mi>o</mi><mi>P</mi><mi>E</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">⋅</mo><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">⋅</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex"> RoPE(x_i) = x_i · e^{(i · θ)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.088em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">⋅</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>


<p>特点： 自然包含相对位置关系+非常适合长序列扩展</p>
<h2 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h2><p>Q,K,V 都是 n*d (n个d维的token) 的矩阵，将 n*d切分为h个n*(d&#x2F;h) 的矩阵做 h次atteention计算，再将结果合并，这就是多头注意力机制。<br>为什么多头？ </p>
<ol>
<li>多头注意力让模型能在不同的“注意力子空间”中，同时关注序列里的不同关系模式。（answered by GPT）</li>
<li>多头可以预防不好的initialization，增强模型鲁棒性(answered by Prince)</li>
</ol>
<p>MHA（Multi-Head Attention）：<br>每个 head 都有独立的 Q&#x2F;K&#x2F;V → 功能强、计算贵</p>
<p>MQA（Multi-Query Attention）：<br>每个 head 只有 Q 不同，但 共享一份 K 和一份 V → 大幅降低显存和推理成本，是现代大模型主流。</p>
<h2 id="KV-cache"><a href="#KV-cache" class="headerlink" title="KV-cache"></a>KV-cache</h2><p>在生成任务中，比如 GPT 这种 decoder架构，每次生成一个 token，都需要计算前面所有 token 的 attention，这样计算量会随着生成长度线性增加，导致推理速度变慢。使用一个cache存放已经计算过的K和V可以大大减少计算量和memory。 </p>
<p>Q: 为什么 Query 不缓存？<br>A: 因为 Q（Query）只用在“当前 token”上，所以每次推理都需要新的 Q。缓存没有意义。</p>
<h2 id="fine-tunning"><a href="#fine-tunning" class="headerlink" title="fine_tunning"></a>fine_tunning</h2><p>在预训练模型的基础上，针对具体任务进行微调.</p>
<h3 id="full-fine-tunning"><a href="#full-fine-tunning" class="headerlink" title="full fine-tunning"></a>full fine-tunning</h3><p>对模型所有参数都进行微调，灵活性高但计算成本巨大.</p>
<h3 id="Soft-prompt"><a href="#Soft-prompt" class="headerlink" title="Soft prompt"></a>Soft prompt</h3><p>我们平常输入的prompt叫hard prompt. 这一部分会被tokenizer切割成很多token，然后经过word embedding操作变成[h1,h2,…] (H<em>D 的矩阵)<br>而soft prompt则是一串embedding的向量, [g1,g2,..] (G</em>D 的矩阵). 我们将其输入到模型里，冻结模型的其他所有参数，只通过梯度反向传播更新这个向量。<br>在学习之后，我们就得到了这样一串学好的soft prompt[g1,g2,..]。</p>
<p>然后在输入时，我们将hard prompt和训练好的soft prompt连接（concatenate），把这个向量作为input: [h1,h2,…,g1,g2,…] ( (H+G)*D 的矩阵)<br>相当于输入序列长度变大了(H-&gt;H+G)，而每个token的维度没变 ）， 这样可以让模型对特定任务表现更好。</p>
<p>soft prompt的好处是省显存，只需要更新soft prompt的向量就行了，参数量G*D, 如果像传统方法一样微调所有参数的话，参数量在&gt;&#x3D;7B,非常需要算力。</p>
<h3 id="soft-prefix"><a href="#soft-prefix" class="headerlink" title="soft-prefix"></a>soft-prefix</h3><p>类似soft prompt, 但不是在input的前面加一串向量, 而是加在每一层的attention的K,V里. 但Q不变.<br>soft prompt是多了G个token，因此Q,K,V的序列也都多了G列. 而soft prefix则可以看作多了G个隐藏文本，单个token查询不仅要查输入文本token的k，还有soft-prefix加的隐藏文本的k. 但当前文本长度是不变的，因此Q没有变化. </p>
<h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>Low-Rank Adaptation of Large Language Models.</p>
<p>在模型的某些权重矩阵W上，添加两个低秩矩阵A和B，使得微调时只更新A和B，而冻结W。</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mi>W</mi><mo>∗</mo><mo>+</mo><mi>δ</mi><mi>W</mi><mo separator="true">,</mo><mi>δ</mi><mi>W</mi><mo>=</mo><mi>A</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">W = W*+\delta W, \delta W = AB </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">+</span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>

<p>在 LoRA 中，权重更新矩阵表示为：<br>[<br>\Delta W &#x3D; A \cdot B<br>]</p>
<p>其中：</p>
<ul>
<li>(A \in \mathbb{R}^{d_\text{out} \times r})  </li>
<li>(B \in \mathbb{R}^{r \times d_\text{in}})  </li>
<li>(r) 是低秩（rank），通常远小于 (d_\text{in}, d_\text{out})</li>
</ul>
<h4 id="1-初始化原则"><a href="#1-初始化原则" class="headerlink" title="1. 初始化原则"></a>1. 初始化原则</h4><ol>
<li><p><strong>矩阵 B 初始化为较小的随机值</strong>  </p>
<ul>
<li>目的是让 LoRA 初始输出接近 0，不影响原模型的输出  </li>
<li>常用方法：<ul>
<li>均匀分布：<br>[<br>B_{i,j} \sim \mathcal{U}\Big[-\frac{\alpha}{\sqrt{r}}, \frac{\alpha}{\sqrt{r}}\Big]<br>]<br>其中 (\alpha) 一般为 0.01～0.1</li>
<li>正态分布：<br>[<br>B_{i,j} \sim \mathcal{N}(0, \sigma^2), \quad \sigma &#x3D; 0.01<br>]</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>矩阵 A 初始化为零</strong>  </p>
<ul>
<li>这样 (\Delta W &#x3D; A B \approx 0)  </li>
<li>保证初始模型输出几乎不受 LoRA 影响  </li>
<li>训练时 A 会逐渐学习如何调整权重</li>
</ul>
</li>
</ol>
<h4 id="2-为什么这样初始化"><a href="#2-为什么这样初始化" class="headerlink" title="2. 为什么这样初始化"></a>2. 为什么这样初始化</h4><p>初始 LoRA 输出很小，模型性能不会突然变化; 训练过程中 LoRA 会逐步调整权重，避免梯度爆炸; 可以直接加载预训练模型，不破坏原模型知识  </p>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>encoder结构，学习input的表征(representation)</p>
<p>BERT有SEP token，用于分隔句子。有CLS token，用于分类任务。采用bidirectional attention，即每个token的注意力都考虑到了它前面和后面的所有token。而GPT只采用了unidirectional attention，即每个token只考虑到了它前面的token。</p>
<p>BERT 训练采用 Masked Language Modeling (MLM)：随机遮住词，要求模型利用左右上下文预测. 这是unsupervised training, 不需要人工贴标签,可以用巨量语料学习语言规律, 从而得到一个强大的语言模型.</p>
<h1 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h1><p>传统softmax based attention 的时间复杂度是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>∗</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2 * d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span>， n是序列长度. 计算量主要来自于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex"> QK^T </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>. 每个 token 都要和其他 n 个 token 计算 attention score, 就是拿一个certain token的Q去和序列里所有的token的K做点积(dot product), Q,K是d维的，那么点积这一步就是O(d)的复杂度, 和n个token的K做点积，因此计算certain token的attention score的复杂度是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo>∗</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O (n*d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span>. 总共有 n 个 token, 那么就是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>∗</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2*d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span> 了.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://gty864.github.io">GTY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://gty864.github.io/2025/11/16/Transformer/">http://gty864.github.io/2025/11/16/Transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/11/23/Pytorch/" title="Pytorch"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Pytorch</div></div><div class="info-2"><div class="info-item-1">记录一些乱七八糟的pytorch知识 1nn.Linear(in_features, out_features, bias=True)  创建一个线性变换层(y&#x3D;Wx+b)，输入维度是in_features，输出维度是out_features，bias表示是否使用偏置项，默认为True。 e.g. 1linear = nn.Linear(10, 5)  创建一个输入维度为10，输出维度为5的线性层。 123torch.matmul(A，B)       #matrix multiplicationtorch.matmul(a,b)        #vector dot producttorch.matmul(A,scalar)    #matrix multiply scalar  矩阵乘法，自动选择维度，可以进行向量点积，矩阵乘法，矩阵乘标量。 1K.transpose(-1, -2)     #transpose last two dimensions 转置K的最后两个维度，-1代表最后一个维度，-2代表倒数第二个维度。为什么只转置张量K的最后两个维度? K是一个四...</div></div></div></a><a class="pagination-related" href="/2025/11/12/2024/" title="2024年终总结"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">2024年终总结</div></div><div class="info-2"><div class="info-item-1">没想到居然会在2025年快结束之际在大洋彼岸的凌晨2点写下这篇2024年度总结，感慨万千。 2024是神奇的一年，很多事情在这一年发生了改变。在2024的前一年，2023年，我完成了束缚自己18余年的高考，踏入了大学的校门，迎来了身份的转变。如果说2023年是我的过渡之年的话，2024年则是真正重塑我人格的一年。 我们来看看这一年开头发生了什么。 是的，大学的第一个期末考来了，幸运的是，我的发挥不错，在线代和高数分别获得了A和A+（虽然我觉得我高数考的非常烂）。再加上中通和信导的A，我的第一学期绩点达到了3.9，专业第二。 现在回想起来，第一学期的我是真的能让现在的自己自惭形秽。对比如今混乱的生活作息，当时的我还是保持着12-1点睡8-10点起的正常作息。课上听的也十分认真，笔记每节课都记，虽然相对高中时期摆烂不少，但在大部分刚上大学放飞自我的同学中已是出类拔萃。 我还自己用pygame写了一个类似元气骑士的小游戏，后来在这个模板上改了改搞成鹁鸽包围的半成品游戏。 过年的时候，和jack一起王者冲分，结果双排连跪死活上不去王者，最后jack先上了王者后退游，我输输赢赢n把后怒而退...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/11/11/test/" title="DL"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-11</div><div class="info-item-2">DL</div></div><div class="info-2"><div class="info-item-1">Will be completed in a few days. BLEU: Bilingual Evaluation Understudy 机器翻译与文本生成中最常用的自动评价指标之一，用来衡量模型生成的句子与参考答案（人类写的正确句子）之间的相似度。 Xavier 初始化（Xavier Initialization）1. 背景在训练神经网络时，权重的初始化非常关键。如果初始化不合理，可能出现：  梯度消失（vanishing gradient）：深层网络梯度太小，无法更新权重 梯度爆炸（exploding gradient）：梯度太大，训练不稳定  Xavier 初始化（又叫 Glorot 初始化）旨在解决这个问题，使网络训练更稳定。  2. 核心思想Xavier 的目标是：  保持每一层输出的方差 ≈ 输入的方差    假设：  全连接层权重矩阵 (W) 的维度为 (n_\text{out} \times n_\text{in})   输入 (x)，输出 (y &#x3D; W x)  Xavier 希望：[\text{Var}(y) \approx \text{Var}(...</div></div></div></a><a class="pagination-related" href="/2025/11/23/Pytorch/" title="Pytorch"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-23</div><div class="info-item-2">Pytorch</div></div><div class="info-2"><div class="info-item-1">记录一些乱七八糟的pytorch知识 1nn.Linear(in_features, out_features, bias=True)  创建一个线性变换层(y&#x3D;Wx+b)，输入维度是in_features，输出维度是out_features，bias表示是否使用偏置项，默认为True。 e.g. 1linear = nn.Linear(10, 5)  创建一个输入维度为10，输出维度为5的线性层。 123torch.matmul(A，B)       #matrix multiplicationtorch.matmul(a,b)        #vector dot producttorch.matmul(A,scalar)    #matrix multiply scalar  矩阵乘法，自动选择维度，可以进行向量点积，矩阵乘法，矩阵乘标量。 1K.transpose(-1, -2)     #transpose last two dimensions 转置K的最后两个维度，-1代表最后一个维度，-2代表倒数第二个维度。为什么只转置张量K的最后两个维度? K是一个四...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">GTY</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/gty864"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-number">1.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Word-Embedding"><span class="toc-number">1.1.</span> <span class="toc-text">Word Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Positional-encoding"><span class="toc-number">1.2.</span> <span class="toc-text">Positional encoding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer-XL-%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.2.1.</span> <span class="toc-text">Transformer-XL 类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#T5-%E7%9A%84-Relative-Position-Bias"><span class="toc-number">1.2.2.</span> <span class="toc-text">T5 的 Relative Position Bias</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RoPE%EF%BC%9ARotary-Positional-Embedding%EF%BC%88GPT-LLaMA-%E4%B8%BB%E6%B5%81%E4%BD%BF%E7%94%A8%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">RoPE：Rotary Positional Embedding（GPT&#x2F;LLaMA 主流使用）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-head-Attention"><span class="toc-number">1.3.</span> <span class="toc-text">Multi-head Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KV-cache"><span class="toc-number">1.4.</span> <span class="toc-text">KV-cache</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#fine-tunning"><span class="toc-number">1.5.</span> <span class="toc-text">fine_tunning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#full-fine-tunning"><span class="toc-number">1.5.1.</span> <span class="toc-text">full fine-tunning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Soft-prompt"><span class="toc-number">1.5.2.</span> <span class="toc-text">Soft prompt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#soft-prefix"><span class="toc-number">1.5.3.</span> <span class="toc-text">soft-prefix</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LoRA"><span class="toc-number">1.5.4.</span> <span class="toc-text">LoRA</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8E%9F%E5%88%99"><span class="toc-number">1.5.4.1.</span> <span class="toc-text">1. 初始化原则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E6%A0%B7%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.5.4.2.</span> <span class="toc-text">2. 为什么这样初始化</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BERT"><span class="toc-number">2.</span> <span class="toc-text">BERT</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">3.</span> <span class="toc-text">复杂度</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/23/Pytorch/" title="Pytorch">Pytorch</a><time datetime="2025-11-24T05:47:47.000Z" title="Created 2025-11-23 21:47:47">2025-11-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/16/Transformer/" title="Transformer">Transformer</a><time datetime="2025-11-17T05:35:37.000Z" title="Created 2025-11-16 21:35:37">2025-11-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/12/2024/" title="2024年终总结">2024年终总结</a><time datetime="2025-11-12T08:17:13.000Z" title="Created 2025-11-12 00:17:13">2025-11-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/11/test/" title="DL">DL</a><time datetime="2025-11-12T05:47:47.000Z" title="Created 2025-11-11 21:47:47">2025-11-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/11/hexo-basic/" title="Hexo basic commands">Hexo basic commands</a><time datetime="2025-11-11T08:00:00.000Z" title="Created 2025-11-11 00:00:00">2025-11-11</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By GTY</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="/true"></script></div></body></html>